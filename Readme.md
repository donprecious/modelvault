# üß† MiniVault API ‚Äî ModelVault Take-Home

A lightweight, local REST API that simulates the core feature of ModelVault‚Äôs product: accepting a prompt and returning a generated response via a local LLM. This solution uses **FastAPI** for the API layer and **Ollama** to run lightweight models like `tinyllama:1.1b-chat` ‚Äî all 100% offline.

---

## ‚ú® Features

- ‚úÖ `POST /generate` ‚Äî send a prompt and receive a model-generated response  
- ‚úÖ Logs all interactions to `logs/log.jsonl`  
- ‚úÖ Uses **local LLM only** (no OpenAI or Anthropic)  
- ‚úÖ Dockerized setup with **modular control**  
- ‚úÖ Environment-based model configuration via `.env`  
- ‚ö°Ô∏è Optional WebSocket stream endpoint (`/ws/generate`)  
- üíª Includes CLI for quick local testing  

---

## üß± Directory Structure

```
minivault-api/
‚îú‚îÄ‚îÄ app/                     # FastAPI app code
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Entry point (FastAPI routes)
‚îÇ   ‚îú‚îÄ‚îÄ core.py              # LLM + logger setup
‚îÇ   ‚îî‚îÄ‚îÄ cli.py               # CLI tool for testing prompts
‚îú‚îÄ‚îÄ logs/log.jsonl          # Prompt/response logs
‚îú‚îÄ‚îÄ .env                    # Environment config (model, base URL)
‚îú‚îÄ‚îÄ requirements.txt        # Python dependencies
‚îú‚îÄ‚îÄ Dockerfile              # API container
‚îú‚îÄ‚îÄ docker-compose.yml      # Ollama container
‚îú‚îÄ‚îÄ docker-compose-app.yml  # API container (used after model is pulled)
|‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ postman_collection.json # Postman collection 

```

---

## üöÄ Setup & Usage

### Step 1: Start Ollama (model server)

We start the Ollama container **alone** so we can pull the model first.

```bash
docker compose up -d --build
```

This starts Ollama on `localhost:11434`.

---

### Step 2: Pull a lightweight local model

We use [`tinyllama:1.1b-chat`](https://huggingface.co/codellama) for speed and low memory usage (CPU-friendly).

```bash
docker exec -it ollama ollama pull tinyllama:1.1b-chat
```

‚úÖ This downloads the model into the persistent `ollama_data` volume.

> üß† **Why this manual step?**  
> Pulling a model inside the API container startup causes delays and timeouts because Ollama isn‚Äôt "healthy" until the pull finishes. That‚Äôs why we **separate** the model pull step from API startup ‚Äî for speed and clarity.

---

### Step 3A: Run API locally (for development)

After pulling the model, you can start FastAPI using your host Python:

```bash
# Activate your virtualenv
python -m venv .venv && source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt

# Start the app
uvicorn app.main:app --reload
```

---

### Step 3B: Or run the API via Docker Compose

We use a **second compose file** just for the API:

```bash
docker compose -f docker-compose-app.yml up --build
```

> üí° **Why two compose files?**
>
> - `docker-compose.yml` starts **only Ollama**, so you can pull the model first.
> - `docker-compose-app.yml` starts the **FastAPI app**, after the model is ready.
>
> This avoids model download timeouts during API boot and gives you flexibility.

---

## üîç Example Usage

### ‚úÖ REST API

```http
POST /generate
Content-Type: application/json

{ "prompt": "Explain gravity in one sentence." }
```

Returns:

```json
{ "response": "Gravity is a force of attraction..." }
```

---

### üß™ CLI Testing

```bash
python app/cli.py "Explain photosynthesis"
```

---

### üîå WebSocket Streaming (Optional)

```ws
ws://localhost:8000/ws/generate
```

Send JSON:

```json
{ "prompt": "Tell me a joke." }
```

---

## üì¶ Environment Variables

Configure these in your `.env` file:

```env
OLLAMA_MODEL=tinyllama:1.1b-chat
OLLAMA_BASE_URL=http://localhost:11434
```

---

## üí° Tradeoffs & Improvement Ideas

| Area                      | Current                                      | Potential Upgrade                                                                       |
|---------------------------|----------------------------------------------|-----------------------------------------------------------------------------------------|
| **Model Setup**           | Manual pull of `tinyllama` use advance model | Automate with job queue/bootstrap health script  and use advance model likeDeepSeek-R1  |
| **Prompt Logging**        | Logs to `.jsonl` file                        | Add SQLite or Redis for searchable history                                              |
| **Stateless API**         | No conversation memory                       | Add in-memory or persistent store (e.g., LangChain Memory or sqlite or postgres)        |
| **Authentication**        | None                                         | Add API key/token-based auth                                                            |
| **Testing**               | Manual via Postman or CLI                    | Add Pytest + sample prompt tests                                                        |
| **UI / Playground**       | None                                         | Add Swagger UI or simple chat HTML front                                                |

---

## ‚úÖ Submission Checklist

- [x] Local REST API  
- [x] Logs all interactions  
- [x] Uses a local LLM (`tinyllama`)  
- [x] CLI & WebSocket support  
- [x] Clean modular code with `.env` and Docker  
- [x] README with detailed setup & tradeoffs  

---

